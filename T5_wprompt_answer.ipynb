{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responding to Writing Prompts using T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project we decided to expand our knowledge in Machine Learning by reading papers about transformers. \n",
    "The main papers were:\n",
    "1. https://arxiv.org/abs/1706.03762 (Attention is all you need)\n",
    "2. https://arxiv.org/abs/1910.10683 (Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer)\n",
    "After reading these papers and understanding how a transformer works and what problems is suited to. We decided to apply T5 to a homemade dataset of prompts and stories to see if T5 could write a good story given a prompt. \n",
    "\n",
    "Additionally, for t-5 use, we referenced this medium article: https://towardsdatascience.com/poor-mans-gpt-3-few-shot-text-generation-with-t5-transformer-51f1b01f843e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are using the T5 transformer to generate stories. This is a pretrained transformer that takes test as an input and gives text as an output. We trained the model on writing prompts and their responses from reddit, then we gave the model prompts to see what kinds of stories it could come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing necessary packages. After this, you need to restart the kernel.\n",
    "#!pip install -quiet transformers==2.9.0 pmaw seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install  necessary packages for T5\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instance of T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is All You Need was a paper published by Google Research in 2017, which introduced Transformers. Transformers are a deep learning model that utilizes \"Attention\" to track relationships in sequential data. They seperated themselves from a lot of deep learning networks at the time by not needing recurrence or convolution layers. Google Research used their transfomer model to translate English to German with amazing results. This type of model does a lot better on Language tasks than RNNs and CNNs because it can learn from context which makes it easier to remember long sequences. These are the  biggest advantages of transformers though:\n",
    "1. The complexity by layer is O(1) vs O(n) in RNN\n",
    "2. It is parallelizable \n",
    "3. Constant length between long distance dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `\"Transformer.png\"'\n",
      "/bin/bash: -c: line 0: `[title](\"Transformer.png\")'\n"
     ]
    }
   ],
   "source": [
    "![title](\"Transformer.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder is where we convert the input words into vectors. We assign numbers to all the words based off there similarity using Positional Encoders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "These vectors are then passed into a Multi-Head Attention layer. The Multi-Head Attention layer is based off the concept of \"self-attention\" which finds words of importance in each sentence. Each word gets an attention score based off a mathematical operation and that is how the network prioritizes words. There are multiple attention vectors per words and take a weighted average to compute a final attention vector for each word hence why it is \"Multi-Head\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The final big piece of a Transformer is the Decoder. The Decoder has the same position encoding as the Encoder, the model predicts the next output in a sequence and tweaks its weights based off if the answer was wrong or right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping to Get Training Data\n",
    "In this section we will be scraping data from the subreddit /r/WritingPrompts. The unique thing about this is WritingPrompts has posts that are prompts for people in the comments to respond to. This will give us a bunch of prompt-response pairs. We are going to try and train T5 by giving it prompts with however many responses to see how it would respond to a new prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imorting packages webscraping and cleaning data\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "import praw\n",
    "from datetime import date, timedelta,datetime\n",
    "from time import sleep\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as plticker\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits=['WritingPrompts']\n",
    "reddit = praw.Reddit(client_id=\"clnrV4XDQaihDQ\",      # your client id\n",
    "                     client_secret=\"hp9vKzrUzsIrE4YmMoaePoKj2h4BrA\",  #your client secret\n",
    "                     user_agent=\"my user agent\", #user agent name\n",
    "                     username = \"Ok_Researcher2247\",     # your reddit username\n",
    "                     password = \"kxbf3puk\")     # your reddit password\n",
    "api = PushshiftAPI(num_workers=8,praw=reddit)\n",
    "def comments_to_json(comments):\n",
    "    body=[]\n",
    "    coms=[]\n",
    "    id=[]\n",
    "    for comm in comments:\n",
    "        body.append(comm['title'])\n",
    "        id.append(comm['id'])\n",
    "        coms.append(comm['num_comments'])\n",
    "    dict = {'Comment':body,'Num_Coms':coms,'ID':id} \n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "posts = api.search_submissions(subreddit=\"WritingPrompts\", num_comments='>5',limit=100)\n",
    "post_list = [post for post in posts]\n",
    "# for post in post_list:\n",
    "#     print(post)\n",
    "posts=comments_to_json(post_list)\n",
    "df=pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 6 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 1 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 24 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 5 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 2 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 4 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 21 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 10 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 16 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 3 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pmaw/Request.py:230: UserWarning: 9 items were not found in Pushshift\n",
      "  f'{self.limit} items were not found in Pushshift')\n"
     ]
    }
   ],
   "source": [
    "ids=list(df['ID'])\n",
    "comms=[]\n",
    "for id in ids:\n",
    "    comment_ids = api.search_submission_comment_ids(ids=id)\n",
    "    comment_id_list = [c_id for c_id in comment_ids]\n",
    "    com_ids=[]\n",
    "    for com in comment_id_list:\n",
    "        com_ids.append(com['id'])\n",
    "    comments = api.search_comments(ids=com_ids)\n",
    "    comment_list = [comment for comment in comments]\n",
    "    raw_comms=[]\n",
    "    for comment in comment_list:\n",
    "        if len(comment['body'])>1500 and comment['body'][0]!='*':\n",
    "            raw_comms.append(comment['body'])\n",
    "    comms.append(raw_comms)\n",
    "df['Comments']=comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Num_Coms</th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PM] Give me prompts!</td>\n",
       "      <td>51</td>\n",
       "      <td>o71peo</td>\n",
       "      <td>[The rodent’s head snapped like a dry twig.\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[WP] A tall, abandoned tower suddenly appears ...</td>\n",
       "      <td>4</td>\n",
       "      <td>o70qqu</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[WP] You are playing DND and roll a Nat 20 for...</td>\n",
       "      <td>7</td>\n",
       "      <td>o6zvnm</td>\n",
       "      <td>[Damin Nox looked up from the table where he a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[PI] As opposed to getting rid of the creepy d...</td>\n",
       "      <td>12</td>\n",
       "      <td>o6z73h</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[WP] Write about a mundane inconvenience with ...</td>\n",
       "      <td>7</td>\n",
       "      <td>o6z11z</td>\n",
       "      <td>[Hi u/Door_Knight, this submission has been re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[WP] Ancient letters reveal the 2nd Amendment ...</td>\n",
       "      <td>6</td>\n",
       "      <td>o4ua40</td>\n",
       "      <td>[The voicemail Frank received simply stated, “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[WP] The Hero and Princess are marrying. The c...</td>\n",
       "      <td>25</td>\n",
       "      <td>o4u8yk</td>\n",
       "      <td>[“If anyone has any objections, speak now or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[WP] It took millions of years for faster than...</td>\n",
       "      <td>7</td>\n",
       "      <td>o4tq26</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[SP] Slowly, the moon began to fall.</td>\n",
       "      <td>6</td>\n",
       "      <td>o4oysh</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[WP] Years ago, the last remaining God of the ...</td>\n",
       "      <td>92</td>\n",
       "      <td>o4nquw</td>\n",
       "      <td>[Osiris sat on his throne, alone in a dead cav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comment  Num_Coms      ID  \\\n",
       "0                               [PM] Give me prompts!        51  o71peo   \n",
       "1   [WP] A tall, abandoned tower suddenly appears ...         4  o70qqu   \n",
       "2   [WP] You are playing DND and roll a Nat 20 for...         7  o6zvnm   \n",
       "3   [PI] As opposed to getting rid of the creepy d...        12  o6z73h   \n",
       "4   [WP] Write about a mundane inconvenience with ...         7  o6z11z   \n",
       "..                                                ...       ...     ...   \n",
       "95  [WP] Ancient letters reveal the 2nd Amendment ...         6  o4ua40   \n",
       "96  [WP] The Hero and Princess are marrying. The c...        25  o4u8yk   \n",
       "97  [WP] It took millions of years for faster than...         7  o4tq26   \n",
       "98               [SP] Slowly, the moon began to fall.         6  o4oysh   \n",
       "99  [WP] Years ago, the last remaining God of the ...        92  o4nquw   \n",
       "\n",
       "                                             Comments  \n",
       "0   [The rodent’s head snapped like a dry twig.\\n ...  \n",
       "1                                                  []  \n",
       "2   [Damin Nox looked up from the table where he a...  \n",
       "3                                                  []  \n",
       "4   [Hi u/Door_Knight, this submission has been re...  \n",
       "..                                                ...  \n",
       "95  [The voicemail Frank received simply stated, “...  \n",
       "96  [“If anyone has any objections, speak now or f...  \n",
       "97                                                 []  \n",
       "98                                                 []  \n",
       "99  [Osiris sat on his throne, alone in a dead cav...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pandas/core/frame.py:4908: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/pandas/core/frame.py:5042: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#cleaning data\n",
    "df['Comments2']=df['Comments'].apply(lambda x: ' '.join(x))\n",
    "df=df[df['Comments2']!='']\n",
    "df.drop([\"Comments2\", \"Num_Coms\", \"ID\"],axis=1,inplace=True)\n",
    "df.rename(columns={\"Comment\":\"Prompts\"},inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "temp=df['Prompts']\n",
    "to_add=[]\n",
    "for sent in temp:\n",
    "    temp=sent[4:]\n",
    "    to_add.append(temp)\n",
    "df['Prompts']=to_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompts</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me prompts!</td>\n",
       "      <td>[The rodent’s head snapped like a dry twig.\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are playing DND and roll a Nat 20 for per...</td>\n",
       "      <td>[Damin Nox looked up from the table where he a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write about a mundane inconvenience with the ...</td>\n",
       "      <td>[Hi u/Door_Knight, this submission has been re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have been sentenced to death in a magical...</td>\n",
       "      <td>[“Toddle Nozzletinker, you have been found gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a secret agent searching for a crimin...</td>\n",
       "      <td>[\"God damnit command, this place has more obsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>The year is 2061. Technology is a must-have e...</td>\n",
       "      <td>[The world turned to gray, then blue, red, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>You are a farmer that created a simple shrine...</td>\n",
       "      <td>[Oswald walked along briskly, keeping a nervou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Ancient letters reveal the 2nd Amendment was ...</td>\n",
       "      <td>[The voicemail Frank received simply stated, “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The Hero and Princess are marrying. The cerem...</td>\n",
       "      <td>[“If anyone has any objections, speak now or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Years ago, the last remaining God of the Univ...</td>\n",
       "      <td>[Osiris sat on his throne, alone in a dead cav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompts  \\\n",
       "0                                    Give me prompts!   \n",
       "1    You are playing DND and roll a Nat 20 for per...   \n",
       "2    Write about a mundane inconvenience with the ...   \n",
       "3    You have been sentenced to death in a magical...   \n",
       "4    You are a secret agent searching for a crimin...   \n",
       "..                                                ...   \n",
       "65   The year is 2061. Technology is a must-have e...   \n",
       "66   You are a farmer that created a simple shrine...   \n",
       "67   Ancient letters reveal the 2nd Amendment was ...   \n",
       "68   The Hero and Princess are marrying. The cerem...   \n",
       "69   Years ago, the last remaining God of the Univ...   \n",
       "\n",
       "                                             Comments  \n",
       "0   [The rodent’s head snapped like a dry twig.\\n ...  \n",
       "1   [Damin Nox looked up from the table where he a...  \n",
       "2   [Hi u/Door_Knight, this submission has been re...  \n",
       "3   [“Toddle Nozzletinker, you have been found gui...  \n",
       "4   [\"God damnit command, this place has more obsc...  \n",
       "..                                                ...  \n",
       "65  [The world turned to gray, then blue, red, and...  \n",
       "66  [Oswald walked along briskly, keeping a nervou...  \n",
       "67  [The voicemail Frank received simply stated, “...  \n",
       "68  [“If anyone has any objections, speak now or f...  \n",
       "69  [Osiris sat on his throne, alone in a dead cav...  \n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of comments: 4048.4545454545455\n",
      "Maximum length of comments: 9965\n",
      "Average length of prompts: 50.86181818181818\n",
      "Maximum length of prompts: 296\n"
     ]
    }
   ],
   "source": [
    "#analyiizing the scraped data\n",
    "coms=[i for sub in df['Comments'] for i in sub]\n",
    "avg=0\n",
    "std=0\n",
    "max=0\n",
    "for i in range(len(coms)):\n",
    "    avg+=len(coms[i])\n",
    "    if len(coms[i])>max:\n",
    "        max=len(coms[i])\n",
    "\n",
    "avg/=len(coms)\n",
    "print(f'Average length of comments: {avg}')\n",
    "print(f'Maximum length of comments: {max}')\n",
    "\n",
    "prompts=df['Prompts'].tolist()\n",
    "avg=0\n",
    "std=0\n",
    "max=0\n",
    "for i in range(len(prompts)):\n",
    "    avg+=len(prompts[i])\n",
    "    if len(prompts[i])>max:\n",
    "        max=len(prompts[i])\n",
    "\n",
    "avg/=len(coms)\n",
    "print(f'Average length of prompts: {avg}')\n",
    "print(f'Maximum length of prompts: {max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, Some Clean Data, now, lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompts</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me prompts!</td>\n",
       "      <td>[The rodent’s head snapped like a dry twig.\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are playing DND and roll a Nat 20 for per...</td>\n",
       "      <td>[Damin Nox looked up from the table where he a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write about a mundane inconvenience with the ...</td>\n",
       "      <td>[Hi u/Door_Knight, this submission has been re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have been sentenced to death in a magical...</td>\n",
       "      <td>[“Toddle Nozzletinker, you have been found gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a secret agent searching for a crimin...</td>\n",
       "      <td>[\"God damnit command, this place has more obsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“I’m sorry…” the hero sobbed in front of a lo...</td>\n",
       "      <td>[The night was overcast, rain imminent. Past m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sun is dead, and we killed it.</td>\n",
       "      <td>[A SHORT STORY\\n\\n  \\n\"It was for the greater ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>You're the town's blacksmith. A mysterious lo...</td>\n",
       "      <td>[“Heartwood of ancient yew, a dram of powdered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A mere month away from moving in together, yo...</td>\n",
       "      <td>[“His eyes are so dreamy.”\\r  \\n\\r  \\nThese we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Your cranky neighbor is actually an immortal ...</td>\n",
       "      <td>[Every day was tiring, between school and work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>You're a hero who was just brutally betrayed ...</td>\n",
       "      <td>[After retching out the dirt and bugs, I surve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>You are an NPC character in a flower shop of ...</td>\n",
       "      <td>[The baker sat in the flower shop. It was alot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>During a sweltering triple digit summer after...</td>\n",
       "      <td>[Feldonwurst Creek, a sleepy little town named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The zombie apocalypse started around 3 years ...</td>\n",
       "      <td>[If he was anything he was relieved. The flesh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Your party didn't care about you. You were th...</td>\n",
       "      <td>[\"Wait, Perry? Is that you?\" Was Cassandra's b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>You are a world-renowned assassin, and you al...</td>\n",
       "      <td>[I had positioned myself on the rooftops, abov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Your spouse is the great overlord, the owner ...</td>\n",
       "      <td>[He destroyed their world. I saw it trough the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Theres nothing we’ve ever done that my twin s...</td>\n",
       "      <td>[##Lunar Twins\\n\\nMy husband sleeps in the cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>You are a highly skilled necromancer. You don...</td>\n",
       "      <td>[The fields were glowing golden in the warm af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Humanity opens a portal to another dimension ...</td>\n",
       "      <td>[Styx was an oft-forgotten backwater world on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Tired of society and people you run to the fo...</td>\n",
       "      <td>[It was supposed to be the following day. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Earth isn't a \"death world\" it's paradise. Hu...</td>\n",
       "      <td>[“And to your right...another TECHNICOLOUR WAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>An angel and a devil sitting on their human's...</td>\n",
       "      <td>[\"Whatever it takes,\" Taylor muttered to herse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>You were finally pressured into cleaning out ...</td>\n",
       "      <td>[Why does my life suck so much? First the Nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Officially, you're a weak, D rank villain. Un...</td>\n",
       "      <td>[The wind whistling past my ears and rushing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A person finds a computer, and upon opening u...</td>\n",
       "      <td>[\\[Zero Waiting\\]\\n\\n\"Well that went...,\" Oz s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>You have many house plants and would often si...</td>\n",
       "      <td>[One of my most two embarrassing moments both ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>You are a demon who ran away from hell and de...</td>\n",
       "      <td>[“I’m not even mad about the car. Hell knows I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Immortality is not what people thought it wou...</td>\n",
       "      <td>[I've been on Spaceout a lot lately.  Probably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"You can go to Kaelara to live, but you canno...</td>\n",
       "      <td>[Aw man, another day at the ol'salt mines.\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"Dark and light are magical affinities, not m...</td>\n",
       "      <td>[“...Come again?” A young man who carried an a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>When the zombie apocalypse hits, you have to ...</td>\n",
       "      <td>[I saw my wife, Diane, off to New York City ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>\"I was so mad, the laws of physics stopped wo...</td>\n",
       "      <td>[God sat down with a disgruntled thud and a si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>You're a secret agent investigating fraud wit...</td>\n",
       "      <td>[It was not with a heavy sigh that Tony closed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>For years you've been hunting a myth, a creat...</td>\n",
       "      <td>[He was just like the other conspiracy nuts-- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Every beginning of night you get ready and ma...</td>\n",
       "      <td>[Drifting clouds of yellow and green floated b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>You have successfully created a super intelli...</td>\n",
       "      <td>[Thalia sighed as she wrote another log in her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>You are an immortal monarch whose only wish i...</td>\n",
       "      <td>[Drinking from the Fountain of Youth had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>“So this is it huh? The end of the world….it’...</td>\n",
       "      <td>[From the observation deck of the HMRS Baylon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Every planet humanity has been to, the Sailor...</td>\n",
       "      <td>[Vlad stood up with a big smile on his face.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>This section of the cemetery has no statues, ...</td>\n",
       "      <td>[Old Friend\\n\\nRain poured heavily on the outs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Talking Tuesday (Tasks): Comedy &amp; Reflecting ...</td>\n",
       "      <td>[Okay, so the weakness that I mentioned in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The knight had made a mistake. One, the princ...</td>\n",
       "      <td>[“Okay, run me through it one more time.” The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>The shelf is filled with small glass bottles:...</td>\n",
       "      <td>[The bitter yellow concoction made my nose wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>You are a demon. Most people contact you to s...</td>\n",
       "      <td>[Silence reigns as you stare at the entity bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>After successfully containing the anti-carbon...</td>\n",
       "      <td>[This was it. This was the culmination of year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Three menacing delinquents entered the buildi...</td>\n",
       "      <td>[I knew why they were sending me: Because I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>The great deception. Vampires actually love g...</td>\n",
       "      <td>[Vampiriafest, the day when the humans in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Your door is kicked down(literally) as the he...</td>\n",
       "      <td>[Cornelius floated in his personal ship orbiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A new park opened for the wealthy called zomb...</td>\n",
       "      <td>[\\[Strolling through Paradise\\]\\n\\n\"Welcome to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompts  \\\n",
       "0                                    Give me prompts!   \n",
       "1    You are playing DND and roll a Nat 20 for per...   \n",
       "2    Write about a mundane inconvenience with the ...   \n",
       "3    You have been sentenced to death in a magical...   \n",
       "4    You are a secret agent searching for a crimin...   \n",
       "5    “I’m sorry…” the hero sobbed in front of a lo...   \n",
       "6                  The sun is dead, and we killed it.   \n",
       "7    You're the town's blacksmith. A mysterious lo...   \n",
       "8    A mere month away from moving in together, yo...   \n",
       "9    Your cranky neighbor is actually an immortal ...   \n",
       "10   You're a hero who was just brutally betrayed ...   \n",
       "11   You are an NPC character in a flower shop of ...   \n",
       "12   During a sweltering triple digit summer after...   \n",
       "13   The zombie apocalypse started around 3 years ...   \n",
       "14   Your party didn't care about you. You were th...   \n",
       "15   You are a world-renowned assassin, and you al...   \n",
       "16   Your spouse is the great overlord, the owner ...   \n",
       "17   Theres nothing we’ve ever done that my twin s...   \n",
       "18   You are a highly skilled necromancer. You don...   \n",
       "19   Humanity opens a portal to another dimension ...   \n",
       "20   Tired of society and people you run to the fo...   \n",
       "21   Earth isn't a \"death world\" it's paradise. Hu...   \n",
       "22   An angel and a devil sitting on their human's...   \n",
       "23   You were finally pressured into cleaning out ...   \n",
       "24   Officially, you're a weak, D rank villain. Un...   \n",
       "25   A person finds a computer, and upon opening u...   \n",
       "26   You have many house plants and would often si...   \n",
       "27   You are a demon who ran away from hell and de...   \n",
       "28   Immortality is not what people thought it wou...   \n",
       "29   \"You can go to Kaelara to live, but you canno...   \n",
       "30   \"Dark and light are magical affinities, not m...   \n",
       "31   When the zombie apocalypse hits, you have to ...   \n",
       "32   \"I was so mad, the laws of physics stopped wo...   \n",
       "33   You're a secret agent investigating fraud wit...   \n",
       "34   For years you've been hunting a myth, a creat...   \n",
       "35   Every beginning of night you get ready and ma...   \n",
       "36   You have successfully created a super intelli...   \n",
       "37   You are an immortal monarch whose only wish i...   \n",
       "38   “So this is it huh? The end of the world….it’...   \n",
       "39   Every planet humanity has been to, the Sailor...   \n",
       "40   This section of the cemetery has no statues, ...   \n",
       "41   Talking Tuesday (Tasks): Comedy & Reflecting ...   \n",
       "42   The knight had made a mistake. One, the princ...   \n",
       "43   The shelf is filled with small glass bottles:...   \n",
       "44   You are a demon. Most people contact you to s...   \n",
       "45   After successfully containing the anti-carbon...   \n",
       "46   Three menacing delinquents entered the buildi...   \n",
       "47   The great deception. Vampires actually love g...   \n",
       "48   Your door is kicked down(literally) as the he...   \n",
       "49   A new park opened for the wealthy called zomb...   \n",
       "\n",
       "                                             Comments  \n",
       "0   [The rodent’s head snapped like a dry twig.\\n ...  \n",
       "1   [Damin Nox looked up from the table where he a...  \n",
       "2   [Hi u/Door_Knight, this submission has been re...  \n",
       "3   [“Toddle Nozzletinker, you have been found gui...  \n",
       "4   [\"God damnit command, this place has more obsc...  \n",
       "5   [The night was overcast, rain imminent. Past m...  \n",
       "6   [A SHORT STORY\\n\\n  \\n\"It was for the greater ...  \n",
       "7   [“Heartwood of ancient yew, a dram of powdered...  \n",
       "8   [“His eyes are so dreamy.”\\r  \\n\\r  \\nThese we...  \n",
       "9   [Every day was tiring, between school and work...  \n",
       "10  [After retching out the dirt and bugs, I surve...  \n",
       "11  [The baker sat in the flower shop. It was alot...  \n",
       "12  [Feldonwurst Creek, a sleepy little town named...  \n",
       "13  [If he was anything he was relieved. The flesh...  \n",
       "14  [\"Wait, Perry? Is that you?\" Was Cassandra's b...  \n",
       "15  [I had positioned myself on the rooftops, abov...  \n",
       "16  [He destroyed their world. I saw it trough the...  \n",
       "17  [##Lunar Twins\\n\\nMy husband sleeps in the cha...  \n",
       "18  [The fields were glowing golden in the warm af...  \n",
       "19  [Styx was an oft-forgotten backwater world on ...  \n",
       "20  [It was supposed to be the following day. The ...  \n",
       "21  [“And to your right...another TECHNICOLOUR WAS...  \n",
       "22  [\"Whatever it takes,\" Taylor muttered to herse...  \n",
       "23  [Why does my life suck so much? First the Nigh...  \n",
       "24  [The wind whistling past my ears and rushing t...  \n",
       "25  [\\[Zero Waiting\\]\\n\\n\"Well that went...,\" Oz s...  \n",
       "26  [One of my most two embarrassing moments both ...  \n",
       "27  [“I’m not even mad about the car. Hell knows I...  \n",
       "28  [I've been on Spaceout a lot lately.  Probably...  \n",
       "29  [Aw man, another day at the ol'salt mines.\\n\\n...  \n",
       "30  [“...Come again?” A young man who carried an a...  \n",
       "31  [I saw my wife, Diane, off to New York City ea...  \n",
       "32  [God sat down with a disgruntled thud and a si...  \n",
       "33  [It was not with a heavy sigh that Tony closed...  \n",
       "34  [He was just like the other conspiracy nuts-- ...  \n",
       "35  [Drifting clouds of yellow and green floated b...  \n",
       "36  [Thalia sighed as she wrote another log in her...  \n",
       "37  [Drinking from the Fountain of Youth had been ...  \n",
       "38  [From the observation deck of the HMRS Baylon ...  \n",
       "39  [Vlad stood up with a big smile on his face.\\n...  \n",
       "40  [Old Friend\\n\\nRain poured heavily on the outs...  \n",
       "41  [Okay, so the weakness that I mentioned in the...  \n",
       "42  [“Okay, run me through it one more time.” The ...  \n",
       "43  [The bitter yellow concoction made my nose wri...  \n",
       "44  [Silence reigns as you stare at the entity bef...  \n",
       "45  [This was it. This was the culmination of year...  \n",
       "46  [I knew why they were sending me: Because I'm ...  \n",
       "47  [Vampiriafest, the day when the humans in the ...  \n",
       "48  [Cornelius floated in his personal ship orbiti...  \n",
       "49  [\\[Strolling through Paradise\\]\\n\\n\"Welcome to...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short = df.head(50)\n",
    "\n",
    "df_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1\n",
      "epoch  2\n",
      "epoch  3\n",
      "epoch  4\n",
      "epoch  5\n",
      "epoch  6\n",
      "epoch  7\n",
      "epoch  8\n",
      "epoch  9\n"
     ]
    }
   ],
   "source": [
    "#training the model on 50 prompts\n",
    "t5_model.train()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print (\"epoch \",epoch)\n",
    "  for id in range(len(df_short)):\n",
    "    for story in df_short['Comments'][id]:\n",
    "      input_sent = \"create story: \"+df_short[\"Prompts\"][id]+ \" </s>\"\n",
    "      ouput_sent = story+\" </s>\"\n",
    "\n",
    "      tokenized_inp = tokenizer.encode_plus(input_sent,  max_length=9133, return_tensors=\"pt\")\n",
    "      tokenized_output = tokenizer.encode_plus(ouput_sent, max_length=300 ,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "      input_ids  = tokenized_inp[\"input_ids\"]\n",
    "      attention_mask = tokenized_inp[\"attention_mask\"]\n",
    "\n",
    "      lm_labels= tokenized_output[\"input_ids\"]\n",
    "      decoder_attention_mask=  tokenized_output[\"attention_mask\"]\n",
    "\n",
    "\n",
    "      # the forward function automatically creates the correct decoder_input_ids\n",
    "      output = t5_model(input_ids=input_ids, lm_labels=lm_labels,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)\n",
    "      loss = output[0]\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here are a few prompts and how our model responds to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliasmann/opt/anaconda3/envs/AppliedMLenv/lib/python3.7/site-packages/transformers/modeling_utils.py:1432: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: create story: The sky rips open and a powerful alien comes to you and tells you to bring him the best croissant in the world, or humanity is doomed. </s>\n",
      "\n",
      "response: [Zero Waiting] \"Well, that's a lie. I've never been one to think of anything like that in my life.\" A powerful alien came to me and told me that it would be great if it were not for the gravitational pull that led him to this place. He would probably have liked to have seen it, but he wouldn't be the first to go, or at least, to experience the plight of an alien being on the other side of the galaxy. Then again, the alien had no value beyond what the human being could bring him the best croissant in the world. \"The greatest salute in history.\" An alien screamed to him, his voice echoing through his thoughts. A small voice came from the heavens, and the voice rang out in his head. It was an entirely different experience for him. There was nothing more to learn from his experience than what it took to send him on his quest to find the perfect place to live or work or study. Or maybe it was more interesting to watch. What would you like to see today was different than the way you'd experience in living or studying and learning to master the art of making things available to the general public. As far as I could tell, there was no better way to survive that would make it possible for me to continue to make that happen. If I were to return to Earth, I would have the option of doing so would I go back to my home world to be able to do so? &#x200B; Or the same universe entirely entirely on its own. And so, when the universe was created entirely new worlds creations would inevitably result in loss of life. That would mean the creation of something new. But beyond the realm of possibility. Something new and different. From the perspective, humanity would not be as diverse as the one that could be unmatched by any other living beings on this planet, we would all just experiment with this new life as we could possibly hope for. We would just need to try out new things, maybe even learn to impose new ones as soon as possible. Whether it be through the cracks and re-create something like this one? The answer is simple: \"I will bring it to you first, it will be different for all of us.\" The answers are endless.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = \"The sky rips open and a powerful alien comes to you and tells you to bring him the best croissant in the world, or humanity is doomed.\"\n",
    "test_sent = \"create story: \"+p+ \" </s>\"\n",
    "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
    "\n",
    "test_input_ids  = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "\n",
    "t5_model.eval()\n",
    "beam_outputs = t5_model.generate(\n",
    "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
    "    max_length=500,\n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print(\"prompt: \" + test_sent + \"\\n\")\n",
    "    print (\"response: \" + sent + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: create story:  You work at the cities hospital in the unofficial \"hero ward,\" the wing of the hospital that deals with the cities' numerous vigilantes that come in for medical treatment at odd hours of the night. One day, on a seemingly ordinary shift, twenty or so vigilantes come in at once. </s>\n",
      "\n",
      "response: The unofficial \"hero ward,\" the wing of the hospital that deals with the city's numerous vigilantes that come in for medical treatment. Twenty-seven people, on a seemingly ordinary shift, came in at the same time. The hospital, which housed the hospitals' dozens of patients, treated in batches of batches. Each batch consisted of: doctors, nurses, paramedics, etc. They all had to be well-trained to ensure that the patients received the proper care and treatment they needed. One was for the heroes whose names were assigned to them at random. It was also known as the \"Hero Ward\"; that is, if they had any doubts about whether or not the appropriate hospital is the right place for them to go in the morning hours. There were also days where they didn't have to endure such an unusually long shift. At the time, they would usually end up leaving the day off work or go back to that day. That day was the exception: the fact that most of those days were the case at least half-heartedly and completely ignore them. On the days they often resulted in hospital admissions. As far as I could tell, there were plenty of reasons why they were not treated at all. &#x200B; There was only one day that fewer than twenty-five minutes to complete the assignment. Of course, the number of cases that could be considered unsatisfied by the doctors and hospitals that cared only to have them retrench their powers entirely entirely on their behalf. In the event of an inconvenient circumstances that led to the impending events that required immediate action. So, in lieu of medical attention, patients would not be able to see the full hospital. And so, one patient would go unanswered. But on the second or third day, doctors would see twenty or so cases where the patient was not covered by insurance companies that dealt with cases of care for people who had not responded by simply refusing to pay their medical bills. Sometimes they'd be held accountable for not being held responsible for bringing back their loved ones to return to their respective hospitals as soon as they could. Some chose not to stay away from the system entirely. Other than that, some decided to keep their right to remain at their disposal and continue their treatment and come back\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sent = \"create story: \"+df[\"Prompts\"][52]+ \" </s>\"\n",
    "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
    "\n",
    "test_input_ids  = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "\n",
    "t5_model.eval()\n",
    "beam_outputs = t5_model.generate(\n",
    "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
    "    max_length=500,\n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print(\"prompt: \" + test_sent + \"\\n\")\n",
    "    print (\"response: \" + sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: create story: Turns out, true AI exist, and are actually pretty common. The only reason we don’t know about them is because they manifest in and are contained within video games, usually sprouting from an NPC and being stuck with their limited capabilities. Your the first to influence the code and escape. </s>\n",
      "\n",
      "response: It turns out, true AI exist, and are actually quite common. The only reason we don't know about them is because they sprout in and out of video games. They are, in fact, a lot more complex than we think they might be. So, when we first heard of them, we were the first to notice that they were actually real. We were going to have to figure out how to manipulate them. [Strolling through](https://www.reddit.com/r/WritingPrompts/comments/wiki//?context=1) They were created specifically for this purpose. It was our first step to be able to grow their size and can be used to make them available for free. This is when they become available, they became available to people who cared enough to maintain their originality. In this case, AI was the main reason for being open source. If we had the choice of not using them would be forced to pay for their use. There is no limit to their ability to control their behavior and thus, the only way to do this is through the use of their limited resources and to create new ones. As far as we can tell. These are the most powerful AIs we have ever seen in the world. But they also have access to them via the means of production and use them effectively. And they are also very good at converting them into something else entirely entirely on their own. Of course, it is always fun to try and test their limits and see if they actually help us and help them to realize their full potential. Is it possible to replicate them in real life. For example, I would like to experiment with them and make sure they don’t interfere with their creations they’re used for. What exactly are they for and how they work. You’ll find out what they do with the original creators as soon as they come up with new ideas for improvement. Not to worry, though. Try to think outside of the box, so try to find the source code before trying something new, real AI. Make sure to test them out. Test them before they fail, testing them for themselves. Testing them once they arrive, test it before it becomes impossible to counteract the effects they impose on the game. Check out the impact they have on someone else? Test their abilities, tests and tests they will\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"create story: \"+df[\"Prompts\"][53]+ \" </s>\"\n",
    "test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
    "\n",
    "test_input_ids  = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "\n",
    "t5_model.eval()\n",
    "beam_outputs = t5_model.generate(\n",
    "    input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
    "    max_length=500,\n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "for beam_output in beam_outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print(\"prompt: \" + test_sent + \"\\n\")\n",
    "    print (\"response: \" + sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As you can see, the model's storys do not always make sense. This model is not very good at writing stories. However, the themes from the prompts are still there, and definitely learned some literary techniques from the reddit authors. However, the model was only trained on 50 writing prompts for the sake of training time, and given the small dataset, it is impressive that it can generate stories an the level ability that it has. It has almost nailed down sentence structure and use of quotes.\n",
    "\n",
    "I would like to fine tune the loss function in this model further, and train it on a much larger dataset to see what kind of results it will yeild. I beleive that with enough data, it will be able to write good stories. Well... at least good for Reddit. The model performs much better that whhen it was trained only 5 propmts so a bump from 50 to 500 or evemn 5000 prompts would be extremely interesting to see."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f9cea7c49783f1fc7b44157107d925b008f247db1fd1068691671db8e91201a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('AppliedMLenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
